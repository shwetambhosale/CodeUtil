{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.devops.connection import Connection\n",
    "from msrest.authentication import BasicAuthentication\n",
    "import os\n",
    "\n",
    "# Load environment variables (if using .env to store sensitive information)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set up your personal access token (PAT) and organization URL\n",
    "personal_access_token = os.getenv(\"AZURE_DEVOPS_PAT\")  # Store your PAT in a .env file\n",
    "organization_url = 'https://dev.azure.com/shwetambhosale18'\n",
    "\n",
    "# Authenticate using the personal access token\n",
    "credentials = BasicAuthentication('', personal_access_token)\n",
    "connection = Connection(base_url=organization_url, creds=credentials)\n",
    "\n",
    "# Initialize the Git client\n",
    "git_client = connection.clients.get_git_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_from_repo(repo_name, project_name, file_types=['.py', '.js']):\n",
    "    repo = git_client.get_repository(project=project_name, repository_id=repo_name)\n",
    "    \n",
    "    # Get items (files and directories) from the repository root\n",
    "    items = git_client.get_items(project=project_name, repository_id=repo.id, recursion_level='Full')\n",
    "\n",
    "    if not items:\n",
    "        print(\"No items found in the repository.\")\n",
    "    \n",
    "    code = \"\"\n",
    "    for item in items:\n",
    "        if item.is_folder:\n",
    "            print(f\"Skipping folder: {item.path}\")\n",
    "        else:\n",
    "            print(f\"Found file: {item.path}\")  # Debug print to show found file paths\n",
    "            \n",
    "            if any(item.path.endswith(ext) for ext in file_types):  # Filter by file type\n",
    "                print(f\"Fetching content of: {item.path}\")  # Debug print to show file being processed\n",
    "                \n",
    "                # Get the content of each item (handle as a generator)\n",
    "                file_content_generator = git_client.get_blob_content(project=project_name, repository_id=repo.id, sha1=item.object_id)\n",
    "                \n",
    "                # Collect content from the generator and decode bytes\n",
    "                file_content = ''.join([chunk.decode('utf-8') for chunk in file_content_generator])\n",
    "\n",
    "                if file_content:\n",
    "                    code += file_content  # Append the content to the `code` string\n",
    "                else:\n",
    "                    print(f\"No content found for file: {item.path}\")  # Debug print if no content is found\n",
    "    \n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping folder: /\n",
      "Found file: /README.md\n",
      "Found file: /app.py\n",
      "Fetching content of: /app.py\n",
      "Found file: /index.js\n",
      "Fetching content of: /index.js\n",
      "import streamlit as st\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
      "import pandas as pd\n",
      "import pickle\n",
      "\n",
      "# Load the trained model\n",
      "model = tf.keras.models.load_model('model.h5')\n",
      "\n",
      "# Load the encoders and scaler\n",
      "with open('label_encoder_gender.pkl', 'rb') as file:\n",
      "    label_encoder_gender = pickle.load(file)\n",
      "\n",
      "with open('onehot_encoder_geo.pkl', 'rb') as file:\n",
      "    onehot_encoder_geo = pickle.load(file)\n",
      "\n",
      "with open('scaler.pkl', 'rb') as file:\n",
      "    scaler = pickle.load(file)\n",
      "\n",
      "\n",
      "## streamlit app\n",
      "st.title('Customer Churn PRediction')\n",
      "\n",
      "# User input\n",
      "geography = st.selectbox('Geography', onehot_encoder_geo.categories_[0])\n",
      "gender = st.selectbox('Gender', label_encoder_gender.classes_)\n",
      "age = st.slider('Age', 18, 92)\n",
      "balance = st.number_input('Balance')\n",
      "credit_score = st.number_input('Credit Score')\n",
      "estimated_salary = st.number_input('Estimated Salary')\n",
      "tenure = st.slider('Tenure', 0, 10)\n",
      "num_of_products = st.slider('Number of Products', 1, 4)\n",
      "has_cr_card = st.selectbox('Has Credit Card', [0, 1])\n",
      "is_active_member = st.selectbox('Is Active Member', [0, 1])\n",
      "\n",
      "# Prepare the input data\n",
      "input_data = pd.DataFrame({\n",
      "    'CreditScore': [credit_score],\n",
      "    'Gender': [label_encoder_gender.transform([gender])[0]],\n",
      "    'Age': [age],\n",
      "    'Tenure': [tenure],\n",
      "    'Balance': [balance],\n",
      "    'NumOfProducts': [num_of_products],\n",
      "    'HasCrCard': [has_cr_card],\n",
      "    'IsActiveMember': [is_active_member],\n",
      "    'EstimatedSalary': [estimated_salary]\n",
      "})\n",
      "\n",
      "# One-hot encode 'Geography'\n",
      "geo_encoded = onehot_encoder_geo.transform([[geography]]).toarray()\n",
      "geo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\n",
      "\n",
      "# Combine one-hot encoded columns with input data\n",
      "input_data = pd.concat([input_data.reset_index(drop=True), geo_encoded_df], axis=1)\n",
      "\n",
      "# Scale the input data\n",
      "input_data_scaled = scaler.transform(input_data)\n",
      "\n",
      "\n",
      "# Predict churn\n",
      "prediction = model.predict(input_data_scaled)\n",
      "prediction_proba = prediction[0][0]\n",
      "\n",
      "st.write(f'Churn Probability: {prediction_proba:.2f}')\n",
      "\n",
      "if prediction_proba > 0.5:\n",
      "    st.write('The customer is likely to churn.')\n",
      "else:\n",
      "    st.write('The customer is not likely to churn.')\n",
      "import express from 'express';\n",
      "import cors from 'cors';\n",
      "import dotenv from 'dotenv';\n",
      "import connectDB from './config/db.js';  \n",
      "import authRoutes from './routes/auth.js';\n",
      "\n",
      "dotenv.config();  \n",
      "connectDB();\n",
      "\n",
      "const app = express();\n",
      "app.use(cors());\n",
      "app.use(express.json());  \n",
      "\n",
      "// Use the routes\n",
      "app.use('/auth', authRoutes);  \n",
      "\n",
      "const PORT = process.env.PORT || 3000;\n",
      "app.listen(PORT, () => {\n",
      "    console.log(`Server running on port ${PORT}`);\n",
      "});\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repo_name = \"SPARK\"\n",
    "project_name = \"SPARK\"  # Replace with your actual project name\n",
    "\n",
    "code_df = get_code_from_repo(repo_name, project_name)\n",
    "\n",
    "print(code_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# # Character wise splitting \n",
    "def code_to_chunks(code_text):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=500,\n",
    "        separator=\"\\n\",\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(code_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"import streamlit as st\\r\\nimport numpy as np\\r\\nimport tensorflow as tf\\r\\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\\r\\nimport pandas as pd\\r\\nimport pickle\\r\\n\\r\\n# Load the trained model\\r\\nmodel = tf.keras.models.load_model('model.h5')\\r\\n\\r\\n# Load the encoders and scaler\\r\\nwith open('label_encoder_gender.pkl', 'rb') as file:\\r\\n    label_encoder_gender = pickle.load(file)\\r\\n\\r\\nwith open('onehot_encoder_geo.pkl', 'rb') as file:\\r\\n    onehot_encoder_geo = pickle.load(file)\\r\\n\\r\\nwith open('scaler.pkl', 'rb') as file:\\r\\n    scaler = pickle.load(file)\\r\\n\\r\\n\\r\\n## streamlit app\\r\\nst.title('Customer Churn PRediction')\\r\\n\\r\\n# User input\\r\\ngeography = st.selectbox('Geography', onehot_encoder_geo.categories_[0])\\r\\ngender = st.selectbox('Gender', label_encoder_gender.classes_)\\r\\nage = st.slider('Age', 18, 92)\\r\\nbalance = st.number_input('Balance')\\r\\ncredit_score = st.number_input('Credit Score')\\r\\nestimated_salary = st.number_input('Estimated Salary')\\r\\ntenure = st.slider('Tenure', 0, 10)\\r\\nnum_of_products = st.slider('Number of Products', 1, 4)\\r\\nhas_cr_card = st.selectbox('Has Credit Card', [0, 1])\\r\\nis_active_member = st.selectbox('Is Active Member', [0, 1])\\r\\n\\r\\n# Prepare the input data\\r\\ninput_data = pd.DataFrame({\\r\\n    'CreditScore': [credit_score],\\r\\n    'Gender': [label_encoder_gender.transform([gender])[0]],\\r\\n    'Age': [age],\\r\\n    'Tenure': [tenure],\\r\\n    'Balance': [balance],\\r\\n    'NumOfProducts': [num_of_products],\\r\\n    'HasCrCard': [has_cr_card],\\r\\n    'IsActiveMember': [is_active_member],\\r\\n    'EstimatedSalary': [estimated_salary]\\r\\n})\\r\\n\\r\\n# One-hot encode 'Geography'\\r\\ngeo_encoded = onehot_encoder_geo.transform([[geography]]).toarray()\\r\\ngeo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\\r\\n\\r\\n# Combine one-hot encoded columns with input data\\r\\ninput_data = pd.concat([input_data.reset_index(drop=True), geo_encoded_df], axis=1)\\r\\n\\r\\n# Scale the input data\\r\\ninput_data_scaled = scaler.transform(input_data)\\r\\n\\r\\n\\r\\n# Predict churn\",\n",
       " \"'EstimatedSalary': [estimated_salary]\\r\\n})\\r\\n\\r\\n# One-hot encode 'Geography'\\r\\ngeo_encoded = onehot_encoder_geo.transform([[geography]]).toarray()\\r\\ngeo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\\r\\n\\r\\n# Combine one-hot encoded columns with input data\\r\\ninput_data = pd.concat([input_data.reset_index(drop=True), geo_encoded_df], axis=1)\\r\\n\\r\\n# Scale the input data\\r\\ninput_data_scaled = scaler.transform(input_data)\\r\\n\\r\\n\\r\\n# Predict churn\\r\\nprediction = model.predict(input_data_scaled)\\r\\nprediction_proba = prediction[0][0]\\r\\n\\r\\nst.write(f'Churn Probability: {prediction_proba:.2f}')\\r\\n\\r\\nif prediction_proba > 0.5:\\r\\n    st.write('The customer is likely to churn.')\\r\\nelse:\\r\\n    st.write('The customer is not likely to churn.')\\r\\nimport express from 'express';\\r\\nimport cors from 'cors';\\r\\nimport dotenv from 'dotenv';\\r\\nimport connectDB from './config/db.js';  \\r\\nimport authRoutes from './routes/auth.js';\\r\\n\\r\\ndotenv.config();  \\r\\nconnectDB();\\r\\n\\r\\nconst app = express();\\r\\napp.use(cors());\\r\\napp.use(express.json());  \\r\\n\\r\\n// Use the routes\\r\\napp.use('/auth', authRoutes);  \\r\\n\\r\\nconst PORT = process.env.PORT || 3000;\\r\\napp.listen(PORT, () => {\\r\\n    console.log(`Server running on port ${PORT}`);\\r\\n});\"]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_chunks = code_to_chunks(code_df)\n",
    "code_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_functions_and_classes(code):\n",
    "    tree = ast.parse(code)\n",
    "    chunks = []\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "            # Collect function or class as a chunk\n",
    "            chunks.append(ast.unparse(node))  # Use `ast.unparse` to get the code as a string\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code_chunks = extract_functions_and_classes(python_content)\n",
    "# code_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a863b8ee7a4d588be9dd69758c669d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shwet\\.cache\\huggingface\\hub\\models--microsoft--codebert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece50d16445844bf99b715c7081b506c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e284bc85640449139477ca9556cc2d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da68af467d6c4f1292ddd11af08b2e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47a993b0cb24b49a9ca52a8dc8aa75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b334701666cd4361848810f58a221f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_code_into_functions(source_code):\n",
    "#     # Basic splitting based on function definitions\n",
    "#     functions = re.split(r'(def\\s+\\w+\\(.*\\):)', source_code)\n",
    "    \n",
    "#     # Group the function definitions with their bodies\n",
    "#     result = []\n",
    "#     for i in range(1, len(functions), 2):\n",
    "#         func_def = functions[i].strip()\n",
    "#         func_body = functions[i + 1].strip()\n",
    "#         result.append(f\"{func_def}\\n{func_body}\")\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming `code` is your source code string\n",
    "# chunks = split_code_into_functions(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Number of chunks: {len(chunks)}\")\n",
    "# print(chunks)  # Optional: Print some chunks to check their content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for chunk in code_chunks:\n",
    "    # Tokenize the chunk\n",
    "    inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Average the embeddings to get a fixed-size vector\n",
    "    mean_embeddings = torch.mean(embeddings, dim=1).squeeze()\n",
    "\n",
    "    # Store the mean embeddings\n",
    "    all_embeddings.append(mean_embeddings)\n",
    "\n",
    "# Convert to tensor or numpy array if needed\n",
    "all_embeddings = torch.stack(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3766,  0.0786,  0.3071,  ..., -0.8388, -0.4118,  0.5670],\n",
       "        [-0.3252, -0.0103,  0.2725,  ..., -0.9283, -0.3949,  0.5797]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss  # Make sure to import faiss\n",
    "import numpy as np\n",
    "\n",
    "def get_code_vectorstore(chunks):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Tokenize the chunk\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Get the embeddings from the last hidden state\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "        # Average the embeddings to get a fixed-size vector\n",
    "        mean_embeddings = torch.mean(embeddings, dim=1).squeeze()\n",
    "\n",
    "        # Store the mean embeddings\n",
    "        all_embeddings.append(mean_embeddings)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    embeddings = torch.stack(all_embeddings).detach().numpy()  # Convert to numpy array\n",
    "\n",
    "    # Define the dimension of your embeddings\n",
    "    embedding_dim = embeddings.shape[1]\n",
    "\n",
    "    # Create a FAISS index\n",
    "    index = faiss.IndexFlatL2(embedding_dim)  # Use L2 distance for similarity search\n",
    "\n",
    "    # Add the embeddings to the index\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Now you can return the index along with the texts\n",
    "    return index, chunks  # Return the index and the original chunks for reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = get_code_vectorstore(code_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x000002078251AD30> >,\n",
       " [\"import streamlit as st\\r\\nimport numpy as np\\r\\nimport tensorflow as tf\\r\\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\\r\\nimport pandas as pd\\r\\nimport pickle\\r\\n\\r\\n# Load the trained model\\r\\nmodel = tf.keras.models.load_model('model.h5')\\r\\n\\r\\n# Load the encoders and scaler\\r\\nwith open('label_encoder_gender.pkl', 'rb') as file:\\r\\n    label_encoder_gender = pickle.load(file)\\r\\n\\r\\nwith open('onehot_encoder_geo.pkl', 'rb') as file:\\r\\n    onehot_encoder_geo = pickle.load(file)\\r\\n\\r\\nwith open('scaler.pkl', 'rb') as file:\\r\\n    scaler = pickle.load(file)\\r\\n\\r\\n\\r\\n## streamlit app\\r\\nst.title('Customer Churn PRediction')\\r\\n\\r\\n# User input\\r\\ngeography = st.selectbox('Geography', onehot_encoder_geo.categories_[0])\\r\\ngender = st.selectbox('Gender', label_encoder_gender.classes_)\\r\\nage = st.slider('Age', 18, 92)\\r\\nbalance = st.number_input('Balance')\\r\\ncredit_score = st.number_input('Credit Score')\\r\\nestimated_salary = st.number_input('Estimated Salary')\\r\\ntenure = st.slider('Tenure', 0, 10)\\r\\nnum_of_products = st.slider('Number of Products', 1, 4)\\r\\nhas_cr_card = st.selectbox('Has Credit Card', [0, 1])\\r\\nis_active_member = st.selectbox('Is Active Member', [0, 1])\\r\\n\\r\\n# Prepare the input data\\r\\ninput_data = pd.DataFrame({\\r\\n    'CreditScore': [credit_score],\\r\\n    'Gender': [label_encoder_gender.transform([gender])[0]],\\r\\n    'Age': [age],\\r\\n    'Tenure': [tenure],\\r\\n    'Balance': [balance],\\r\\n    'NumOfProducts': [num_of_products],\\r\\n    'HasCrCard': [has_cr_card],\\r\\n    'IsActiveMember': [is_active_member],\\r\\n    'EstimatedSalary': [estimated_salary]\\r\\n})\\r\\n\\r\\n# One-hot encode 'Geography'\\r\\ngeo_encoded = onehot_encoder_geo.transform([[geography]]).toarray()\\r\\ngeo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\\r\\n\\r\\n# Combine one-hot encoded columns with input data\\r\\ninput_data = pd.concat([input_data.reset_index(drop=True), geo_encoded_df], axis=1)\\r\\n\\r\\n# Scale the input data\\r\\ninput_data_scaled = scaler.transform(input_data)\\r\\n\\r\\n\\r\\n# Predict churn\",\n",
       "  \"'EstimatedSalary': [estimated_salary]\\r\\n})\\r\\n\\r\\n# One-hot encode 'Geography'\\r\\ngeo_encoded = onehot_encoder_geo.transform([[geography]]).toarray()\\r\\ngeo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\\r\\n\\r\\n# Combine one-hot encoded columns with input data\\r\\ninput_data = pd.concat([input_data.reset_index(drop=True), geo_encoded_df], axis=1)\\r\\n\\r\\n# Scale the input data\\r\\ninput_data_scaled = scaler.transform(input_data)\\r\\n\\r\\n\\r\\n# Predict churn\\r\\nprediction = model.predict(input_data_scaled)\\r\\nprediction_proba = prediction[0][0]\\r\\n\\r\\nst.write(f'Churn Probability: {prediction_proba:.2f}')\\r\\n\\r\\nif prediction_proba > 0.5:\\r\\n    st.write('The customer is likely to churn.')\\r\\nelse:\\r\\n    st.write('The customer is not likely to churn.')\\r\\nimport express from 'express';\\r\\nimport cors from 'cors';\\r\\nimport dotenv from 'dotenv';\\r\\nimport connectDB from './config/db.js';  \\r\\nimport authRoutes from './routes/auth.js';\\r\\n\\r\\ndotenv.config();  \\r\\nconnectDB();\\r\\n\\r\\nconst app = express();\\r\\napp.use(cors());\\r\\napp.use(express.json());  \\r\\n\\r\\n// Use the routes\\r\\napp.use('/auth', authRoutes);  \\r\\n\\r\\nconst PORT = process.env.PORT || 3000;\\r\\napp.listen(PORT, () => {\\r\\n    console.log(`Server running on port ${PORT}`);\\r\\n});\"])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
