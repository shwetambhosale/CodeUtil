{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "import re \n",
    "from azure.devops.connection import Connection\n",
    "from msrest.authentication import BasicAuthentication\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from langchain_community.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#PAT\n",
    "personal_access_token = os.getenv(\"AZURE_DEVOPS_PAT\")  \n",
    "organization_url = 'https://dev.azure.com/shwetambhosale18'\n",
    "\n",
    "# Authenticate using the personal access token\n",
    "credentials = BasicAuthentication('', personal_access_token)\n",
    "connection = Connection(base_url=organization_url, creds=credentials)\n",
    "\n",
    "# Initialize the Git client\n",
    "git_client = connection.clients.get_git_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_from_repo(repo_name, project_name, file_types=['.py', '.js']):\n",
    "    repo = git_client.get_repository(project=project_name, repository_id=repo_name)\n",
    "    \n",
    "    # Get items (files and directories) from the repository root\n",
    "    items = git_client.get_items(project=project_name, repository_id=repo.id, recursion_level='Full')\n",
    "\n",
    "    if not items:\n",
    "        print(\"No items found in the repository.\")\n",
    "    \n",
    "    code = \"\"\n",
    "    for item in items:\n",
    "        if item.is_folder:\n",
    "            print(f\"Skipping folder: {item.path}\")\n",
    "        else:\n",
    "            print(f\"Found file: {item.path}\")  # Debug print to show found file paths\n",
    "            \n",
    "            if any(item.path.endswith(ext) for ext in file_types):  # Filter by file type\n",
    "                print(f\"Fetching content of: {item.path}\")  # Debug print to show file being processed\n",
    "                \n",
    "                # Get the content of each item (handle as a generator)\n",
    "                file_content_generator = git_client.get_blob_content(project=project_name, repository_id=repo.id, sha1=item.object_id)\n",
    "                \n",
    "                # Collect content from the generator and decode bytes\n",
    "                file_content = ''.join([chunk.decode('utf-8') for chunk in file_content_generator])\n",
    "\n",
    "                if file_content:\n",
    "                    code += file_content  # Append the content to the `code` string\n",
    "                else:\n",
    "                    print(f\"No content found for file: {item.path}\")  # Debug print if no content is found\n",
    "    \n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping folder: /\n",
      "Found file: /README.md\n",
      "Found file: /app.py\n",
      "Fetching content of: /app.py\n",
      "Found file: /index.js\n",
      "Fetching content of: /index.js\n",
      "import streamlit as st\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
      "import pandas as pd\n",
      "import pickle\n",
      "\n",
      "# Load the trained model\n",
      "model = tf.keras.models.load_model('model.h5')\n",
      "\n",
      "# Load the encoders and scaler\n",
      "with open('label_encoder_gender.pkl', 'rb') as file:\n",
      "    label_encoder_gender = pickle.load(file)\n",
      "\n",
      "with open('onehot_encoder_geo.pkl', 'rb') as file:\n",
      "    onehot_encoder_geo = pickle.load(file)\n",
      "\n",
      "with open('scaler.pkl', 'rb') as file:\n",
      "    scaler = pickle.load(file)\n",
      "\n",
      "\n",
      "## streamlit app\n",
      "st.title('Customer Churn PRediction')\n",
      "\n",
      "# User input\n",
      "geography = st.selectbox('Geography', onehot_encoder_geo.categories_[0])\n",
      "gender = st.selectbox('Gender', label_encoder_gender.classes_)\n",
      "age = st.slider('Age', 18, 92)\n",
      "balance = st.number_input('Balance')\n",
      "credit_score = st.number_input('Credit Score')\n",
      "estimated_salary = st.number_input('Estimated Salary')\n",
      "tenure = st.slider('Tenure', 0, 10)\n",
      "num_of_products = st.slider('Number of Products', 1, 4)\n",
      "has_cr_card = st.selectbox('Has Credit Card', [0, 1])\n",
      "is_active_member = st.selectbox('Is Active Member', [0, 1])\n",
      "\n",
      "# Prepare the input data\n",
      "input_data = pd.DataFrame({\n",
      "    'CreditScore': [credit_score],\n",
      "    'Gender': [label_encoder_gender.transform([gender])[0]],\n",
      "    'Age': [age],\n",
      "    'Tenure': [tenure],\n",
      "    'Balance': [balance],\n",
      "    'NumOfProducts': [num_of_products],\n",
      "    'HasCrCard': [has_cr_card],\n",
      "    'IsActiveMember': [is_active_member],\n",
      "    'EstimatedSalary': [estimated_salary]\n",
      "})\n",
      "\n",
      "# One-hot encode 'Geography'\n",
      "geo_encoded = onehot_encoder_geo.transform([[geography]]).toarray()\n",
      "geo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\n",
      "\n",
      "# Combine one-hot encoded columns with input data\n",
      "input_data = pd.concat([input_data.reset_index(drop=True), geo_encoded_df], axis=1)\n",
      "\n",
      "# Scale the input data\n",
      "input_data_scaled = scaler.transform(input_data)\n",
      "\n",
      "\n",
      "# Predict churn\n",
      "prediction = model.predict(input_data_scaled)\n",
      "prediction_proba = prediction[0][0]\n",
      "\n",
      "st.write(f'Churn Probability: {prediction_proba:.2f}')\n",
      "\n",
      "if prediction_proba > 0.5:\n",
      "    st.write('The customer is likely to churn.')\n",
      "else:\n",
      "    st.write('The customer is not likely to churn.')\n",
      "import express from 'express';\n",
      "import cors from 'cors';\n",
      "import dotenv from 'dotenv';\n",
      "import connectDB from './config/db.js';  \n",
      "import authRoutes from './routes/auth.js';\n",
      "\n",
      "dotenv.config();  \n",
      "connectDB();\n",
      "\n",
      "const app = express();\n",
      "app.use(cors());\n",
      "app.use(express.json());  \n",
      "\n",
      "// Use the routes\n",
      "app.use('/auth', authRoutes);  \n",
      "\n",
      "const PORT = process.env.PORT || 3000;\n",
      "app.listen(PORT, () => {\n",
      "    console.log(`Server running on port ${PORT}`);\n",
      "});\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repo_name = \"SPARK\"\n",
    "project_name = \"SPARK\"  # Replace with your actual project name\n",
    "\n",
    "code_df = get_code_from_repo(repo_name, project_name)\n",
    "\n",
    "print(code_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# # Character wise splitting \n",
    "def code_to_chunks(code_text):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=500,\n",
    "        separator=\"\\n\",\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(code_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"import streamlit as st\\r\\nimport numpy as np\\r\\nimport tensorflow as tf\\r\\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\\r\\nimport pandas as pd\\r\\nimport pickle\\r\\n\\r\\n# Load the trained model\\r\\nmodel = tf.keras.models.load_model('model.h5')\\r\\n\\r\\n# Load the encoders and scaler\\r\\nwith open('label_encoder_gender.pkl', 'rb') as file:\\r\\n    label_encoder_gender = pickle.load(file)\\r\\n\\r\\nwith open('onehot_encoder_geo.pkl', 'rb') as file:\\r\\n    onehot_encoder_geo = pickle.load(file)\\r\\n\\r\\nwith open('scaler.pkl', 'rb') as file:\\r\\n    scaler = pickle.load(file)\\r\\n\\r\\n\\r\\n## streamlit app\\r\\nst.title('Customer Churn PRediction')\\r\\n\\r\\n# User input\\r\\ngeography = st.selectbox('Geography', onehot_encoder_geo.categories_[0])\\r\\ngender = st.selectbox('Gender', label_encoder_gender.classes_)\\r\\nage = st.slider('Age', 18, 92)\\r\\nbalance = st.number_input('Balance')\\r\\ncredit_score = st.number_input('Credit Score')\\r\\nestimated_salary = st.number_input('Estimated Salary')\\r\\ntenure = st.slider('Tenure', 0, 10)\\r\\nnum_of_products = st.slider('Number of Products', 1, 4)\\r\\nhas_cr_card = st.selectbox('Has Credit Card', [0, 1])\\r\\nis_active_member = st.selectbox('Is Active Member', [0, 1])\\r\\n\\r\\n# Prepare the input data\\r\\ninput_data = pd.DataFrame({\\r\\n    'CreditScore': [credit_score],\\r\\n    'Gender': [label_encoder_gender.transform([gender])[0]],\\r\\n    'Age': [age],\\r\\n    'Tenure': [tenure],\\r\\n    'Balance': [balance],\\r\\n    'NumOfProducts': [num_of_products],\\r\\n    'HasCrCard': [has_cr_card],\\r\\n    'IsActiveMember': [is_active_member],\\r\\n    'EstimatedSalary': [estimated_salary]\\r\\n})\\r\\n\\r\\n# One-hot encode 'Geography'\\r\\ngeo_encoded = onehot_encoder_geo.transform([[geography]]).toarray()\\r\\ngeo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\\r\\n\\r\\n# Combine one-hot encoded columns with input data\\r\\ninput_data = pd.concat([input_data.reset_index(drop=True), geo_encoded_df], axis=1)\\r\\n\\r\\n# Scale the input data\\r\\ninput_data_scaled = scaler.transform(input_data)\\r\\n\\r\\n\\r\\n# Predict churn\",\n",
       " \"'EstimatedSalary': [estimated_salary]\\r\\n})\\r\\n\\r\\n# One-hot encode 'Geography'\\r\\ngeo_encoded = onehot_encoder_geo.transform([[geography]]).toarray()\\r\\ngeo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\\r\\n\\r\\n# Combine one-hot encoded columns with input data\\r\\ninput_data = pd.concat([input_data.reset_index(drop=True), geo_encoded_df], axis=1)\\r\\n\\r\\n# Scale the input data\\r\\ninput_data_scaled = scaler.transform(input_data)\\r\\n\\r\\n\\r\\n# Predict churn\\r\\nprediction = model.predict(input_data_scaled)\\r\\nprediction_proba = prediction[0][0]\\r\\n\\r\\nst.write(f'Churn Probability: {prediction_proba:.2f}')\\r\\n\\r\\nif prediction_proba > 0.5:\\r\\n    st.write('The customer is likely to churn.')\\r\\nelse:\\r\\n    st.write('The customer is not likely to churn.')\\r\\nimport express from 'express';\\r\\nimport cors from 'cors';\\r\\nimport dotenv from 'dotenv';\\r\\nimport connectDB from './config/db.js';  \\r\\nimport authRoutes from './routes/auth.js';\\r\\n\\r\\ndotenv.config();  \\r\\nconnectDB();\\r\\n\\r\\nconst app = express();\\r\\napp.use(cors());\\r\\napp.use(express.json());  \\r\\n\\r\\n// Use the routes\\r\\napp.use('/auth', authRoutes);  \\r\\n\\r\\nconst PORT = process.env.PORT || 3000;\\r\\napp.listen(PORT, () => {\\r\\n    console.log(`Server running on port ${PORT}`);\\r\\n});\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_chunks = code_to_chunks(code_df)\n",
    "code_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = []\n",
    "\n",
    "for chunk in code_chunks:\n",
    "    # Tokenize the chunk\n",
    "    inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Average the embeddings to get a fixed-size vector\n",
    "    mean_embeddings = torch.mean(embeddings, dim=1).squeeze()\n",
    "\n",
    "    # Store the mean embeddings\n",
    "    all_embeddings.append(mean_embeddings)\n",
    "\n",
    "# Convert to tensor or numpy array if needed\n",
    "all_embeddings = torch.stack(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.9890e-02, -7.9689e-02, -5.2989e-02,  ..., -5.9282e-02,\n",
       "          -5.1422e-01,  5.0983e-01],\n",
       "         [-6.0591e-01,  1.4039e-01, -1.1477e-03,  ..., -8.0542e-01,\n",
       "          -4.9629e-01,  1.1576e+00],\n",
       "         [-4.5256e-01,  5.9092e-01,  5.5482e-01,  ..., -1.0409e+00,\n",
       "          -5.1213e-01,  1.1935e+00],\n",
       "         ...,\n",
       "         [-2.2689e-01, -3.9840e-01,  1.0522e-01,  ..., -3.4415e-01,\n",
       "          -4.7885e-01,  5.6899e-01],\n",
       "         [ 7.3044e-02, -1.9225e-01,  8.3394e-02,  ..., -5.1262e-01,\n",
       "          -5.4995e-01,  6.2564e-01],\n",
       "         [-8.1032e-02, -7.9339e-02, -5.2885e-02,  ..., -6.0924e-02,\n",
       "          -5.1473e-01,  5.1066e-01]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss  # Make sure to import faiss\n",
    "import numpy as np\n",
    "\n",
    "def get_code_vectorstore(chunks):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Tokenize the chunk\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Get the embeddings from the last hidden state\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "        # Average the embeddings to get a fixed-size vector\n",
    "        mean_embeddings = torch.mean(embeddings, dim=1).squeeze()\n",
    "\n",
    "        # Store the mean embeddings\n",
    "        all_embeddings.append(mean_embeddings)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    embeddings = torch.stack(all_embeddings).detach().numpy()  # Convert to numpy array\n",
    "\n",
    "    # Define the dimension of your embeddings\n",
    "    embedding_dim = embeddings.shape[1]\n",
    "\n",
    "    # Create a FAISS index\n",
    "    index = faiss.IndexFlatL2(embedding_dim)  # Use L2 distance for similarity search\n",
    "\n",
    "    # Add the embeddings to the index\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Now you can return the index along with the texts\n",
    "    return index, chunks  # Return the index and the original chunks for reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = get_code_vectorstore(code_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x00000234164CA0A0> >,\n",
       " [\"import streamlit as st\\r\\nimport numpy as np\\r\\nimport tensorflow as tf\\r\\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\\r\\nimport pandas as pd\\r\\nimport pickle\\r\\n\\r\\n# Load the trained model\\r\\nmodel = tf.keras.models.load_model('model.h5')\\r\\n\\r\\n# Load the encoders and scaler\\r\\nwith open('label_encoder_gender.pkl', 'rb') as file:\\r\\n    label_encoder_gender = pickle.load(file)\\r\\n\\r\\nwith open('onehot_encoder_geo.pkl', 'rb') as file:\\r\\n    onehot_encoder_geo = pickle.load(file)\\r\\n\\r\\nwith open('scaler.pkl', 'rb') as file:\\r\\n    scaler = pickle.load(file)\\r\\n\\r\\n\\r\\n## streamlit app\\r\\nst.title('Customer Churn PRediction')\\r\\n\\r\\n# User input\\r\\ngeography = st.selectbox('Geography', onehot_encoder_geo.categories_[0])\\r\\ngender = st.selectbox('Gender', label_encoder_gender.classes_)\\r\\nage = st.slider('Age', 18, 92)\\r\\nbalance = st.number_input('Balance')\\r\\ncredit_score = st.number_input('Credit Score')\\r\\nestimated_salary = st.number_input('Estimated Salary')\\r\\ntenure = st.slider('Tenure', 0, 10)\\r\\nnum_of_products = st.slider('Number of Products', 1, 4)\\r\\nhas_cr_card = st.selectbox('Has Credit Card', [0, 1])\\r\\nis_active_member = st.selectbox('Is Active Member', [0, 1])\\r\\n\\r\\n# Prepare the input data\\r\\ninput_data = pd.DataFrame({\\r\\n    'CreditScore': [credit_score],\\r\\n    'Gender': [label_encoder_gender.transform([gender])[0]],\\r\\n    'Age': [age],\\r\\n    'Tenure': [tenure],\\r\\n    'Balance': [balance],\\r\\n    'NumOfProducts': [num_of_products],\\r\\n    'HasCrCard': [has_cr_card],\\r\\n    'IsActiveMember': [is_active_member],\\r\\n    'EstimatedSalary': [estimated_salary]\\r\\n})\\r\\n\\r\\n# One-hot encode 'Geography'\\r\\ngeo_encoded = onehot_encoder_geo.transform([[geography]]).toarray()\\r\\ngeo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\\r\\n\\r\\n# Combine one-hot encoded columns with input data\\r\\ninput_data = pd.concat([input_data.reset_index(drop=True), geo_encoded_df], axis=1)\\r\\n\\r\\n# Scale the input data\\r\\ninput_data_scaled = scaler.transform(input_data)\\r\\n\\r\\n\\r\\n# Predict churn\",\n",
       "  \"'EstimatedSalary': [estimated_salary]\\r\\n})\\r\\n\\r\\n# One-hot encode 'Geography'\\r\\ngeo_encoded = onehot_encoder_geo.transform([[geography]]).toarray()\\r\\ngeo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\\r\\n\\r\\n# Combine one-hot encoded columns with input data\\r\\ninput_data = pd.concat([input_data.reset_index(drop=True), geo_encoded_df], axis=1)\\r\\n\\r\\n# Scale the input data\\r\\ninput_data_scaled = scaler.transform(input_data)\\r\\n\\r\\n\\r\\n# Predict churn\\r\\nprediction = model.predict(input_data_scaled)\\r\\nprediction_proba = prediction[0][0]\\r\\n\\r\\nst.write(f'Churn Probability: {prediction_proba:.2f}')\\r\\n\\r\\nif prediction_proba > 0.5:\\r\\n    st.write('The customer is likely to churn.')\\r\\nelse:\\r\\n    st.write('The customer is not likely to churn.')\\r\\nimport express from 'express';\\r\\nimport cors from 'cors';\\r\\nimport dotenv from 'dotenv';\\r\\nimport connectDB from './config/db.js';  \\r\\nimport authRoutes from './routes/auth.js';\\r\\n\\r\\ndotenv.config();  \\r\\nconnectDB();\\r\\n\\r\\nconst app = express();\\r\\napp.use(cors());\\r\\napp.use(express.json());  \\r\\n\\r\\n// Use the routes\\r\\napp.use('/auth', authRoutes);  \\r\\n\\r\\nconst PORT = process.env.PORT || 3000;\\r\\napp.listen(PORT, () => {\\r\\n    console.log(`Server running on port ${PORT}`);\\r\\n});\"])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
